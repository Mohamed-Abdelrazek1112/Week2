{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK8GETFLnVk1"
   },
   "source": [
    "# Week 2: Parsing, Relation Extration and Open Information Extraction\n",
    "### COMP61332: Text Mining, Department of Computer Science, University of Manchester (Riza Batista-Navarro and Viktor Schlegel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_Q5znAUnVk6"
   },
   "source": [
    "In this lab session, you will try out some Python code based on the **spaCy** library (https://spacy.io/) for the NLP tasks discussed in the Week 2 Lecture, as well as an application of NLP (Open Information Extraction or Open IE).\n",
    "After this session, you should be able to:\n",
    "- apply **part-of-speech (POS) tagging** on text\n",
    "- apply **dependency parsing** on text\n",
    "- develop rules for **extracting relations** from text\n",
    "- develop rules for **extracting Open Information Extraction (Open IE) triples**\n",
    "- explore and visualise knowledge extracted by Open IE in the form of a graph (optional)\n",
    "\n",
    "You are provided with three text files (drawn from https://en.wikipedia.org/wiki/Timeline_of_historic_inventions), each containing a list of inventions (from the 1700s, 1800s and 1900s), that you can use for experimentation.\n",
    "\n",
    "<font color=\"red\">NOTE: If you are using Google Colab, upload the three text files using its in-built file browser.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfCB9lHJnVk7"
   },
   "source": [
    "## Preparation of necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35417,
     "status": "ok",
     "timestamp": 1612435723726,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "OCy8Vp6inVk7",
    "outputId": "c1aae47d-5c0c-4d10-b380-d15b0a7622db"
   },
   "outputs": [],
   "source": [
    "# Loading\n",
    "!pip install spacy==3.0\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import Sentencizer\n",
    "\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy import displacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttLa2KkznVlD"
   },
   "source": [
    "## File loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aa10lslsnVlE"
   },
   "source": [
    "The function below takes as parameter the path to a file containing plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1612441135478,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "GKOjBzB0nVlE"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def load_file(path):\n",
    "    file_text = ''\n",
    "    file = codecs.open(path, 'r', encoding = 'utf-8')\n",
    "    file_lines = file.readlines()\n",
    "    for line in file_lines:\n",
    "        # Text cleaning, remove any whitespace lines\n",
    "        line = line.replace('\\n','')\n",
    "        file_text = file_text + line\n",
    "    file.close()\n",
    "    return file_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTCH3_2nVlF"
   },
   "source": [
    "## Sentence segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PHH66F_nVlF"
   },
   "source": [
    "The code below is the same as the one we used in Week 1. Customise the list of sentence delimiters if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1612441143211,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "Kr83TIsMnVlG",
    "outputId": "0a3f13b1-8423-4ab8-e86f-f53e847b8fd6"
   },
   "outputs": [],
   "source": [
    "# Create a new NLP pipeline, specifying English as the language of interest so that English models are loaded.\n",
    "nlp = English()\n",
    "\n",
    "config = {\"punct_chars\": [\".\"]}\n",
    "\n",
    "# Add the component to the pipeline.\n",
    "sentencizer = nlp.add_pipe('sentencizer', config=config)\n",
    "\n",
    "# Load the contents of a text file; change the parameter to use another/your own text file.\n",
    "text = load_file('1800s.txt')\n",
    "\n",
    "# The following line applies the pipeline (so far only sentence segmentation) on the given text, and stores the result in doc.\n",
    "annotations = nlp(text)\n",
    "\n",
    "# Check the result of sentence segmentation.\n",
    "sents_list = []\n",
    "for sent in annotations.sents:\n",
    "    sents_list.append(sent.text.strip())\n",
    "    \n",
    "# Check how many sentences were produced\n",
    "print('Number of sentences: ', len(sents_list))\n",
    "\n",
    "print('SENTENCE NO.\\tSENTENCE:')\n",
    "for i, sent in enumerate(sents_list):\n",
    "    print(i, '\\t', sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d6vawDxnVlG"
   },
   "source": [
    "## Dependency parsing (with in-built POS tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKqxC5TknVlH"
   },
   "source": [
    "### One example for easier visualisation/debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwljyRgqnVlH"
   },
   "source": [
    "The code below applies a dependency parser on a sentence. For each token, the following attributes are printed: \n",
    "- token.text (the token text)\n",
    "- token.lemma_ (the base form of the token)\n",
    "- token.pos_ (the POS tag according to the Universal Dependencies scheme; see https://universaldependencies.org/u/pos/)\n",
    "- token.tag_ (the POS tag according to the Penn Treebank; see https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "- token_dep_ (the dependency type)\n",
    "- child.text:child.dep_ (a list of dependents; the text and dependency type are displayed for each dependent)\n",
    "\n",
    "Moreoever, a **visualisation** of the tree is displayed, which can be helpful later, when you try to write your own rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "executionInfo": {
     "elapsed": 1069,
     "status": "ok",
     "timestamp": 1612435236457,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "yjuRz6__nVlH",
    "outputId": "de896062-1341-4afa-ccae-a68bd4cd270d"
   },
   "outputs": [],
   "source": [
    "# Example 1\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"Nicolas Appert invents the canning process for food.\")\n",
    "for token in doc:\n",
    "    print(token.text + '\\t' + token.lemma_ + '\\t' + token.pos_ + '\\t' + token.tag_ + '\\t' + token.dep_ + '\\t' + str([child.text + ':' + child.dep_ for child in token.children]))\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Weu0wKPYnVlI"
   },
   "source": [
    "### <font color='red'>Below, write down any observations you have based on the results of dependency parsing. For example, what kind of dependencies should you follow if you want to reach names of inventors or inventions from the main verb of a sentence?</font>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4uu8cDonVlI"
   },
   "source": [
    "## Relation extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG029CEmnVlI"
   },
   "source": [
    "### Extracting relations using rules that process the results of dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQlA0d17nVlI"
   },
   "source": [
    "The code below uses very basic rules to extract **inventor-invention relations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1612441861536,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "IPv0kyf6nVlJ"
   },
   "outputs": [],
   "source": [
    "# Create a class that will store every inventor-invention relation that we extract\n",
    "class Relation:\n",
    "    inventor = ''\n",
    "    invention = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1454,
     "status": "ok",
     "timestamp": 1612435262103,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "kXS6Q6oynVlJ",
    "outputId": "57c10f51-532a-4500-ff2e-8abdf40e661c"
   },
   "outputs": [],
   "source": [
    "relations = []\n",
    "\n",
    "for sent in sents_list:\n",
    "    doc = nlp(sent)\n",
    "    for token in doc:\n",
    "        # For now, check if the root of the tree is a verb that is a variant of \"invent\"\n",
    "        if token.dep_ == 'ROOT' and token.lemma_ == 'invent':\n",
    "            r = Relation()\n",
    "            dependents = token.children\n",
    "            for d in dependents:\n",
    "                if d.dep_ == 'dobj':\n",
    "                    r.invention = d.text\n",
    "                elif d.dep_ == 'nsubj':\n",
    "                    r.inventor = d.text\n",
    "            if r.inventor != '' and r.invention != '':\n",
    "                print('Sentence:', sent)\n",
    "                print(r.inventor, '-', r.invention)\n",
    "                relations.append(r)\n",
    "\n",
    "for r in relations:\n",
    "    print(r.inventor, '-', r.invention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuUCs7sHnVlJ"
   },
   "source": [
    "### <font color='red'>The code below is the same as the one above. How will you extend the rules in order to improve the extracted relations? For example: (1) how can you handle verbs that are synonymous to \"invent\"? (2) How will you handle sentences written in the passive voice, e.g., \"X was invented by Y\"? </font>\n",
    "### <font color='red'>You can test your idea and code using the above sample code \"Example 1\" with the sentence \"The canning process for food was invented by Nicolas Appert.\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1612435268079,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "17WARe30nVlJ",
    "outputId": "8d9ab5e3-9b16-4c7a-f96c-14ea697ec80f"
   },
   "outputs": [],
   "source": [
    "relations = []\n",
    "\n",
    "for sent in sents_list:\n",
    "    doc = nlp(sent)\n",
    "    for token in doc:\n",
    "        # For now, check if the root of the tree is a verb that is a variant of \"invent\"\n",
    "        if token.dep_ == 'ROOT' and token.lemma_ == 'invent':\n",
    "            r = Relation()\n",
    "            dependents = token.children\n",
    "            for d in dependents:\n",
    "                if d.dep_ == 'dobj':\n",
    "                    r.invention = d.text\n",
    "                elif d.dep_ == 'nsubj':\n",
    "                    r.inventor = d.text\n",
    "            if r.inventor != '' and r.invention != '':\n",
    "                print('Sentence:', sent)\n",
    "                print(r.inventor, '-', r.invention)\n",
    "                relations.append(r)\n",
    "\n",
    "for r in relations:\n",
    "    print(r.inventor, '-', r.invention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0xuc9sinVlK"
   },
   "source": [
    "### <font color='red'>Summarise below how you extended the code above to improve the extracted relations. In writing your new rules, what other verbs and/or sentence constructions did you consider?</font>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5USsSjVQnVlK"
   },
   "source": [
    "## Open Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ppVfaiFnVlK"
   },
   "source": [
    "### Adapting the code above to generate ARG1-PREDICATE-ARG2 triples instead of relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkVPmiAFnVlK"
   },
   "source": [
    "We can easily adapt the relation extracted code above to extract **ARG1-PREDICATE-ARG2 triples** (for Open Information Extraction) instead of relations. Note that for Open IE, we are interested in any kind of triples (not just those pertaining to inventors and their inventions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1612441852812,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "i5FZZ6I_nVlK"
   },
   "outputs": [],
   "source": [
    "# Example 2\n",
    "# Create a class that will store every ARG1-PREDICATE-ARG2 triple that we extract\n",
    "class Triple:\n",
    "    arg1 = ''\n",
    "    predicate = ''\n",
    "    arg2 = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1372,
     "status": "ok",
     "timestamp": 1612435275213,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "zQeyWZH1nVlK",
    "outputId": "2584bbf3-9e07-4d37-baee-e1eaab145f24"
   },
   "outputs": [],
   "source": [
    "triples = []\n",
    "\n",
    "for sent in sents_list:\n",
    "    doc = nlp(sent)\n",
    "    for token in doc:\n",
    "        # For now, assume that the root of the tree is the predicate\n",
    "        if token.dep_ == 'ROOT':\n",
    "            t = Triple()\n",
    "            # Store the lemmatised form (to normalise)\n",
    "            t.predicate = token.lemma_\n",
    "            dependents = token.children\n",
    "            for d in dependents:\n",
    "                if d.dep_ == 'dobj':\n",
    "                    t.arg2 = d.text\n",
    "                elif d.dep_ == 'nsubj':\n",
    "                    t.arg1 = d.text\n",
    "            if t.arg1 != '' and t.arg2 != '':\n",
    "                print('Sentence:', sent)\n",
    "                print(t.arg1, '-', t.predicate, '-', t.arg2)\n",
    "                triples.append(t)\n",
    "\n",
    "for t in triples:\n",
    "    print(t.arg1, '-', t.predicate, '-', t.arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFQnhWuFnVlK"
   },
   "source": [
    "### <font color='red'>The code below is the same as the one above. How will you extend it to improve the extracted triples? For example, can you try to extract full names of people (rather than just their last names)?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1438,
     "status": "ok",
     "timestamp": 1612435281615,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "rBcqMK4enVlL",
    "outputId": "1cf3c231-454e-4dca-8f66-711893ee25d5"
   },
   "outputs": [],
   "source": [
    "triples = []\n",
    "\n",
    "for sent in sents_list:\n",
    "    doc = nlp(sent)\n",
    "    for token in doc:\n",
    "        # For now, assume that the root of the tree is the predicate\n",
    "        if token.dep_ == 'ROOT':\n",
    "            t = Triple()\n",
    "            # Store the lemmatised form (to normalise)\n",
    "            t.predicate = token.lemma_\n",
    "            dependents = token.children\n",
    "            for d in dependents:\n",
    "                if d.dep_ == 'dobj':\n",
    "                    t.arg2 = d.text\n",
    "                elif d.dep_ == 'nsubj':\n",
    "                    t.arg1 = d.text\n",
    "            if t.arg1 != '' and t.arg2 != '':\n",
    "                print('Sentence:', sent)\n",
    "                print(t.arg1, '-', t.predicate, '-', t.arg2)\n",
    "                triples.append(t)\n",
    "\n",
    "for t in triples:\n",
    "    print(t.arg1, '-', t.predicate, '-', t.arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Do02l9JnVlL"
   },
   "source": [
    "### <font color='red'>Summarise below how you extended the code above to improve the extracted Open IE triples.</font>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oh19cOjanVlM"
   },
   "source": [
    "## Graph representation of knowledge extracted by Open IE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3HGAoHGnVlM"
   },
   "source": [
    "The code below builds a graph representation based on a list of triples extracted by Open IE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhqP95c0nVlM",
    "outputId": "62e89549-d7e3-4aec-8dbc-ab80356f1287"
   },
   "outputs": [],
   "source": [
    "!pip install networkx\n",
    "import networkx as nx\n",
    "\n",
    "# Function for assigning unique IDs to arguments of triples\n",
    "# Returns a dictionary of IDs and the corresponding argument names\n",
    "def normalise_arguments(triples):\n",
    "    global arg_id\n",
    "    # arg_id below serves as the ID\n",
    "    arg_id = 1\n",
    "    argument_dict = dict()\n",
    "    for triple in triples:\n",
    "        argument_dict[triple.arg1] = arg_id\n",
    "        arg_id += 1\n",
    "        argument_dict[triple.arg2] = arg_id\n",
    "        arg_id += 1\n",
    "    return argument_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRkvYjlrnVlN"
   },
   "outputs": [],
   "source": [
    "# Function for generating the graph\n",
    "from typing import List\n",
    "# to_graph fuction was not called/used which will make the code below not running.\n",
    "def to_graph(triples: List[Triple]) -> nx.Graph:\n",
    "    global argument_dict\n",
    "    argument_dict = normalise_arguments(triples)\n",
    "    # Unlike arguments, predicates do not need to be unique\n",
    "    predicate_id = arg_id\n",
    "    graph = nx.Graph()\n",
    "    for triple in triples:\n",
    "        graph.add_node(argument_dict[triple.arg1], name=triple.arg1, type='subject')\n",
    "        graph.add_node(predicate_id, name=triple.predicate, type='predicate')\n",
    "        graph.add_node(argument_dict[triple.arg2], name=triple.arg2, type='object')\n",
    "        graph.add_edge(argument_dict[triple.arg1], predicate_id)\n",
    "        graph.add_edge(argument_dict[triple.arg2], predicate_id)\n",
    "        predicate_id += 1\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yYOguBqnVlN"
   },
   "source": [
    "The code below is for generating a visualisation of a given graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6Rl9qpGnVlN",
    "outputId": "82a2233d-55a4-4cef-bc90-f698495581ec"
   },
   "outputs": [],
   "source": [
    "!pip install pyvis\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from pyvis.network import Network\n",
    "from typing import List\n",
    "\n",
    "class PyVisPrinter:\n",
    "    \"\"\"Class to visualise a (serialized) dataset entry.\"\"\"\n",
    "\n",
    "    def __init__(self, path=None):\n",
    "        self.path = tempfile.mkdtemp(prefix='vis-', dir='/tmp') or path\n",
    "        \n",
    "    def clean(self):\n",
    "        shutil.rmtree(self.path)\n",
    "\n",
    "    def print_graph(self, graph: nx.Graph, filename):\n",
    "\n",
    "        vis = Network(bgcolor=\"#222222\",\n",
    "                      width='100%',\n",
    "                      font_color=\"white\", notebook=True)\n",
    "        \n",
    "        for idx, (node, data) in enumerate(graph.nodes(data=True)):\n",
    "            vis.add_node(\n",
    "                node,\n",
    "                title=data['name'],\n",
    "                label=data['name'],\n",
    "                color='yellow' if data['type'] == 'predicate' else 'green' if data['type'] == 'subject' else 'blue'\n",
    "            )\n",
    "\n",
    "        for i, (source, target) in enumerate(graph.edges()):\n",
    "            if source in vis.get_nodes() and target in vis.get_nodes():\n",
    "                vis.add_edge(source, target)\n",
    "            else:\n",
    "                self.logger.warning(\"source or target not in graph!\")\n",
    "\n",
    "        name = os.path.join(self.path, filename)\n",
    "        return vis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will generate and display an HTML file depicting the graph. \n",
    "<font color=\"red\">NOTE: If you are using Google Colab, the generated HTML will fail to display from within Google Colab. However, you can download the generated HTML file and open it in a separate browser tab/window.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAaLouupnVlO",
    "outputId": "ba9ebfbd-277a-4279-ff4f-cd995163b56a"
   },
   "outputs": [],
   "source": [
    "# Generate a graph based on a list of triples\n",
    "graph = to_graph(triples)\n",
    "\n",
    "# Generate an HTML file with the graph visualisation and display it\n",
    "p = PyVisPrinter()\n",
    "v = p.print_graph(graph, 'my_graph.html')\n",
    "v.show('my_graph.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzUIPEuInVlO"
   },
   "source": [
    "Below is a very basic way to query the graph.\n",
    "The code will return a tree traversed using depth-first search (DFS) with 'pen' as a starting point (answering the question \"Who invented the pen?\").\n",
    "Try other queries by changing the starting point (i.e., change 'pen' to 'Volta' to answer the question: \"What did Volta invent?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME12AYP2nVlO",
    "outputId": "9672e6d3-2928-42bc-8bc8-7cef745d62c7"
   },
   "outputs": [],
   "source": [
    "successors = nx.dfs_tree(graph, argument_dict['pen'])\n",
    "for s in successors:\n",
    "    print (graph.nodes[s])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Week2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
